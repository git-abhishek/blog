---
layout: post
title:  "Starting Open Source Developement"
categories: "gsoc-week1"
permalink: /:categories
---

As my software developer journey is advancing, my love for open source software development is growing. I always wanted to get involved and contribute to open source community. It was not until when I got to know about Google Summer of Code ([GSoC](https://summerofcode.withgoogle.com/)), I had no idea that anyone willing to contribute could get involved with these organizations. 

GSoC acted as a bridge between open source organizations and individuals like me. Through GSoC, I interacted with the community members of [yt](http://yt-project.org/) (a python package for analyzing and visualizing volumetric data). I got excited about their development and this led me to formally apply to yt under GSoC. I am fortunate of being selected by yt and quite enthusiastic about helping in building a tool that has become a part and parcel of many researchers.

During this "summer" I will be focusing on, to enhance and optimize yt's test framework. The main objective is to reduce the test runtime and increase code coverage. With these fixes, I aim to make yt more developer friendly and at the same time minimize the risks due to untested code.

> A developer's job is not just to write test cases. But to have to all path of code executions thoroughly tested.

```
def divide(a,b):
    if b!=0:
        return a/b
   return exit()
 
# Test case when b = 0, is never checked.
# Code coverage could tell us the path
# that never got executed in a complex codebase

def test_divide():
    assert divide(4,2)==2.0
    assert divide(4,-2)==-2.0
```


### 1. Code Coverage

Code coverage is the percentage of code that gets executed when all the test cases corresponding to that piece of code are run. When code coverage is high, it means that most of the code has been rightly tested and we know the expected behavior. Having a metric for code coverage helps in being informed of the key issues with the code base. Further, it helps us identify areas that need improvement. 

For this task, I am using Coveralls which is a tool to analyze code coverage with each pull request (PR). Thus, if the developer has made sure to test all the paths, then code coverage will remain same (or could go higher, if added test cases for old code) otherwise the code coverage will reduce. This makes the developer realize the importance of testing their code thoroughly and ultimately helps in shipping more robust code.

yt has codebase in both Python and Cython. My aim is to improve test suites in both the cases by writing new test cases, if not already done, and to optimize the current test cases. Starting point to expand test cases is by adding support for different geometries (cartesian, cylindrical, and spherical coordinates) and data styles (particle data, mesh data, including uniform resolution, octree, patch AMR, and unstructured meshes). Using the existing test functions fake_random_ds, fake_amr_ds, and fake_particle_ds I can define a fake_test_datasets generators. Using this, a given functionality could be tested for different underlying geometries and data styles.


### 2. Test Runtime

With the increasing codebase, the runtime of tests also increases. If the tests are not written meticulously then this runtime would become linear in the number of test cases. In order to make it efficient, I aim to utilize the parallelization provided by Travis CI [build matrix](https://docs.travis-ci.com/user/customizing-the-build/#Build-Matrix) feature. Using this, we can split the test suite into unit and integration tests resulting in two different build jobs which will run in parallel and fully utilize the available build capacity.

Furthermore, in the case of yt, test runtime could be reduced heavily by improving the answer testing and image comparison tests. Instead of these heavy tests, expected values of the function could be compared with a pre-computed value. Using Matplotlib's image [testing](https://matplotlib.org/1.5.3/devel/testing.html) approach we can compare the gold images (known-correct images) with the generated image ([ImageComparisonTest](https://github.com/matplotlib/matplotlib/blob/5e52ce11ab59176a467dd2c68db8c5e1fbc20a24/lib/matplotlib/testing/decorators.py#L282)) and thus remove the current plot/image answer tests.



With this goal in mind, I warmly welcome all the suggestions and ideas that could be implemented to enhance yt's test framework. I thank all yt community members for helping and guiding me till now and the days to come!
